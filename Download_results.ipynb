{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "import pickle\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape race URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open main results page, capture the html\n",
    "URL = \"https://www.crossresults.com/?n=results&map=0&region=all\"\n",
    "response = requests.get(URL)\n",
    "print(response.status_code)\n",
    "soup = BeautifulSoup(response.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race data get appeneded to each of 4 lists\n",
    "race_path = []\n",
    "race_name = []\n",
    "race_date = []\n",
    "race_region = []\n",
    "\n",
    "# pull race path, name, date and region. \n",
    "# divs belong to 2 classes. Go through each for every month of races\n",
    "for div in soup.find_all(class_=\"monthContent\"):\n",
    "    for tr in div.find_all(class_=\"resultsrow datarow1\"):\n",
    "        race_path.append(tr.find('a').get('href'))\n",
    "        race_name.append(tr.find('a').text)\n",
    "        race_date.append(tr.find('a').findNext().text)\n",
    "        race_region.append(tr.find('a').findNext().findNext().text)\n",
    "    for tr in div.find_all(class_=\"resultsrow datarow2\"):\n",
    "        race_path.append(tr.find('a').get('href'))\n",
    "        race_name.append(tr.find('a').text)\n",
    "        race_date.append(tr.find('a').findNext().text)\n",
    "        race_region.append(tr.find('a').findNext().findNext().text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want races in the US\n",
    "US_regions = ['California', 'Mid Atlantic', 'Mountain West', \n",
    "              'New England', 'New York/Ontario', 'North Central', \n",
    "              'Pacific Northwest', 'South Central', 'Southeast']\n",
    "\n",
    "# These IDs are for US races\n",
    "US_ids = []\n",
    "\n",
    "# Find if region is in the US, save ID if so\n",
    "for idx, region in enumerate(race_region):\n",
    "    if region in US_regions:\n",
    "        US_ids.append(idx)\n",
    "\n",
    "# Update lists to only include US races\n",
    "race_path = [race_path[i] for i in US_ids]\n",
    "race_name = [race_name[i] for i in US_ids]\n",
    "race_date = [race_date[i] for i in US_ids]\n",
    "race_region = [race_region[i] for i in US_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data into one object\n",
    "races = [race_path, race_name, race_date, race_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the race data\n",
    "with open('races.pickle', 'wb') as to_write:\n",
    "    pickle.dump(races, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to do the bulk of the work\n",
    "\n",
    " \n",
    "def lookup_starter_count(row, race_df):\n",
    "    \"\"\"use string in column 'Category Name' to return value from starter_counts\"\"\"\n",
    "    starter_counts = race_df['Category Name'].value_counts()\n",
    "    return starter_counts[row['Category Name']]\n",
    "\n",
    "def lookup_finisher_counts(row, race_df):\n",
    "    \"\"\"Find out how many finishers are in each field and store in dict finisher_counts,\n",
    "    then use string in column 'Category Name' to return value from finisher_counts\"\"\"\n",
    "    # start with an empty dictionary\n",
    "    finisher_counts = {}\n",
    "    # iterate through the categories and placings\n",
    "    for cat, place in zip(race_df['Category Name'], race_df['Place']):\n",
    "        try:\n",
    "            place = int(place)\n",
    "            if cat not in finisher_counts.keys():\n",
    "                finisher_counts[cat] = place\n",
    "            elif finisher_counts[cat] < place:\n",
    "                finisher_counts[cat] = place\n",
    "            else:\n",
    "                pass    \n",
    "        except ValueError:\n",
    "            pass\n",
    "    return finisher_counts[row['Category Name']]\n",
    "\n",
    "\n",
    "def capture_race_data(race_path):\n",
    "    \"\"\"Capture metadata and results from an individual race, \n",
    "    and return a dataframe with that data\"\"\"\n",
    "    \n",
    "    # Get html from individual race page\n",
    "\n",
    "    URL = 'https://www.crossresults.com' + race_path\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "    # Race metadata\n",
    "\n",
    "    main = soup.find(\"div\", {\"id\": \"resultstitle\"}).text.split(' â€¢ ')\n",
    "    # Race Name\n",
    "    name = main[0]\n",
    "    # Race date\n",
    "    date = ' '.join(main[1].split())\n",
    "    # Race location\n",
    "    location = main[2].split('\\r')[0].strip()\n",
    "    # Beers\n",
    "    beers = soup.find(\"div\", {\"class\": \"beerrating rating\"}).text.split()[0]\n",
    "    # Moisture\n",
    "    moisture = soup.find(\"div\", {\"class\": \"moisturerating rating\"}).text.split()[0]\n",
    "    # Accel\n",
    "    accel = soup.find(\"div\", {\"class\": \"accelrating rating\"}).text.split()[0]\n",
    "    # Tech\n",
    "    tech = soup.find(\"div\", {\"class\": \"techrating rating\"}).text.split()[0]\n",
    "    # Elevation\n",
    "    elevation = soup.find(\"div\", {\"class\": \"elevationrating rating\"}).text.split()[0]\n",
    "    # Conditions\n",
    "    conditions = soup.find(\"div\", {\"id\": \"resultstitle\"}).text.strip().split('\\n')[-1].strip()\n",
    "    # Weather\n",
    "    weather = conditions.split(',')[0]\n",
    "    # Temperature\n",
    "    temperature = conditions.split(',')[1].strip().split()[0]\n",
    "    # Wind\n",
    "    wind = conditions.split(',')[2].strip().split()[1]\n",
    "    # extract script tag to get lat and lon\n",
    "    script = soup.find('article', {'id': 'content'}).find('script', {'type': 'text/javascript'})\n",
    "    # pull out the lat and lon\n",
    "    pattern = re.compile('GetMap\\(\\\"(.*?)\"')\n",
    "    try:\n",
    "        lat_lon = re.findall(pattern, script.string)[0]\n",
    "    except IndexError:\n",
    "        lat_lon = np.nan\n",
    "\n",
    "    # Capture results\n",
    "\n",
    "    result_path = soup.find(\"span\", {\"class\": \"downloadoptions\"}).find_all('a')[0]['href']\n",
    "    URL = 'https://www.crossresults.com/' + result_path\n",
    "    response = requests.get(URL)\n",
    "    result_soup = BeautifulSoup(response.text,'lxml')\n",
    "    # read the results into a pandas dataframe\n",
    "    race_df = pd.read_csv(io.StringIO(result_soup.text.strip()), index_col=False)\n",
    "    # only keep the rows that have values in scored points\n",
    "    race_df = race_df.loc[~pd.isnull(race_df['Scored Points'])]\n",
    "\n",
    "    # Run some calculations on results\n",
    "\n",
    "    # calculate points delta\n",
    "    race_df['Points Delta'] = race_df['Scored Points'] - race_df['Carried Points']\n",
    "    # Add column with number of starters in each field\n",
    "    race_df['Starters'] = race_df.apply(lookup_starter_count, race_df = race_df, axis=1)\n",
    "    # Add column with number of finishers in each field\n",
    "    race_df['Finishers'] = race_df.apply(lookup_finisher_counts, race_df = race_df, axis=1)\n",
    "    # Add the race metadata as new columns to the race_df\n",
    "    race_df[['Race Name', 'Date', 'Location', 'Beers', 'Moisture', 'Accel', \n",
    "             'Tech', 'Elevation', 'Weather', 'Temperature', 'Wind', 'Coordinates']] = \\\n",
    "             name, date, location, beers, moisture, accel, \\\n",
    "             tech, elevation, weather, temperature, wind, lat_lon\n",
    "\n",
    "    return race_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the race data\n",
    "with open('races.pickle','rb') as read_file:\n",
    "    races = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_paths = races[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a main df to hold all data from all races\n",
    "all_race_data = pd.DataFrame()\n",
    "\n",
    "for i in range(2):\n",
    "    path = race_paths[i]\n",
    "    all_race_data = all_race_data.append(capture_race_data(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
