{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "import pickle\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to do the bulk of the work\n",
    "\n",
    " \n",
    "def lookup_starter_count(row, race_df):\n",
    "    \"\"\"use string in column 'Category Name' to return value from starter_counts\"\"\"\n",
    "    starter_counts = race_df['Category Name'].value_counts()\n",
    "    return starter_counts[row['Category Name']]\n",
    "\n",
    "def lookup_finisher_counts(row, race_df):\n",
    "    \"\"\"Find out how many finishers are in each field and store in dict finisher_counts,\n",
    "    then use string in column 'Category Name' to return value from finisher_counts\"\"\"\n",
    "    # start with an empty dictionary\n",
    "    finisher_counts = {}\n",
    "    # iterate through the categories and placings\n",
    "    for cat, place in zip(race_df['Category Name'], race_df['Place']):\n",
    "        try:\n",
    "            place = int(place)\n",
    "            if cat not in finisher_counts.keys():\n",
    "                finisher_counts[cat] = place\n",
    "            elif finisher_counts[cat] < place:\n",
    "                finisher_counts[cat] = place\n",
    "            else:\n",
    "                pass    \n",
    "        except ValueError:\n",
    "            pass\n",
    "    return finisher_counts[row['Category Name']]\n",
    "\n",
    "\n",
    "def capture_race_data(race_path):\n",
    "    \"\"\"Capture metadata and results from an individual race, \n",
    "    and return a dataframe with that data\"\"\"\n",
    "    \n",
    "    # Get html from individual race page\n",
    "\n",
    "    URL = 'https://www.crossresults.com' + race_path\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "    # Race metadata\n",
    "\n",
    "    try:\n",
    "        main = soup.find(\"div\", {\"id\": \"resultstitle\"}).text.split(' â€¢ ')\n",
    "    except:\n",
    "        main = np.nan\n",
    "    # Race Name\n",
    "    try:\n",
    "        name = main[0]\n",
    "    except:\n",
    "        name = np.nan\n",
    "    # Race date\n",
    "    try:\n",
    "        date = ' '.join(main[1].split())\n",
    "    except:\n",
    "        date = np.nan\n",
    "    # Race location\n",
    "    try:\n",
    "        location = main[2].split('\\r')[0].strip()\n",
    "    except:\n",
    "        location = np.nan\n",
    "    # Beers\n",
    "    try:\n",
    "        beers = soup.find(\"div\", {\"class\": \"beerrating rating\"}).text.split()[0]\n",
    "    except:\n",
    "        beers = np.nan\n",
    "    # Moisture\n",
    "    try:\n",
    "        moisture = soup.find(\"div\", {\"class\": \"moisturerating rating\"}).text.split()[0]\n",
    "    except:\n",
    "        moisture = np.nan\n",
    "    # Accel\n",
    "    try:\n",
    "        accel = soup.find(\"div\", {\"class\": \"accelrating rating\"}).text.split()[0]\n",
    "    except:\n",
    "        accel = np.nan\n",
    "    # Tech\n",
    "    try:\n",
    "        tech = soup.find(\"div\", {\"class\": \"techrating rating\"}).text.split()[0]\n",
    "    except:\n",
    "        tech = np.nan\n",
    "    # Elevation\n",
    "    try:\n",
    "        elevation = soup.find(\"div\", {\"class\": \"elevationrating rating\"}).text.split()[0]\n",
    "    except:\n",
    "        elevation = np.nan\n",
    "    # Conditions\n",
    "    try:\n",
    "        conditions = soup.find(\"div\", {\"id\": \"resultstitle\"}).text.strip().split('\\n')[-1].strip()\n",
    "    except:\n",
    "        conditions = np.nan\n",
    "    # Weather\n",
    "    try:\n",
    "        weather = conditions.split(',')[0]\n",
    "    except:\n",
    "        weather = np.nan\n",
    "    # Temperature\n",
    "    try:\n",
    "        temperature = conditions.split(',')[1].strip().split()[0]\n",
    "    except:\n",
    "        temperature = np.nan\n",
    "    # Wind\n",
    "    try:\n",
    "        wind = conditions.split(',')[2].strip().split()[1]\n",
    "    except:\n",
    "        wind = np.nan\n",
    "    # extract script tag to get lat and lon\n",
    "    try:\n",
    "        script = soup.find('article', {'id': 'content'}).find('script', {'type': 'text/javascript'})\n",
    "    except:\n",
    "        script = np.nan\n",
    "    # pull out the lat and lon\n",
    "    try: \n",
    "        pattern = re.compile('GetMap\\(\\\"(.*?)\"')\n",
    "    except:\n",
    "        pattern = np.nan\n",
    "    try:\n",
    "        lat_lon = re.findall(pattern, script.string)[0]\n",
    "    except:\n",
    "        lat_lon = np.nan\n",
    "\n",
    "    # Capture results\n",
    "\n",
    "    try:\n",
    "        result_path = soup.find(\"span\", {\"class\": \"downloadoptions\"}).find_all('a')[0]['href']\n",
    "        URL = 'https://www.crossresults.com/' + result_path\n",
    "        response = requests.get(URL)\n",
    "        result_soup = BeautifulSoup(response.text,'lxml')\n",
    "        # read the results into a pandas dataframe\n",
    "        race_df = pd.read_csv(io.StringIO(result_soup.text.strip()), index_col=False)\n",
    "        # only keep the rows that have values in scored points\n",
    "        race_df = race_df.loc[~pd.isnull(race_df['Scored Points'])]\n",
    "    except:\n",
    "        race_df = pd.DataFrame(columns=['Category Name', 'Place', 'RacerID', 'First Name', 'Last Name', 'Team Name',\\\n",
    "                                 'Time', 'License', 'Carried Points', 'Scored Points', 'Points Delta', 'Starters',\\\n",
    "                                 'Finishers', 'Race Name', 'Date', 'Location', 'Beers', 'Moisture', 'Accel',\\\n",
    "                                 'Tech', 'Elevation', 'Weather', 'Temperature', 'Wind', 'Coordinates'])\n",
    "\n",
    "    # As long as there are data in the df...\n",
    "    if len(race_df):\n",
    "    \n",
    "        # Run some calculations on results\n",
    "\n",
    "        # calculate points delta\n",
    "        race_df['Points Delta'] = race_df['Scored Points'] - race_df['Carried Points']\n",
    "        # Add column with number of starters in each field\n",
    "        race_df['Starters'] = race_df.apply(lookup_starter_count, race_df = race_df, axis=1)\n",
    "        # Add column with number of finishers in each field\n",
    "        race_df['Finishers'] = race_df.apply(lookup_finisher_counts, race_df = race_df, axis=1)\n",
    "        # Add the race metadata as new columns to the race_df\n",
    "        race_df[['Race Name', 'Date', 'Location', 'Beers', 'Moisture', 'Accel', \n",
    "                 'Tech', 'Elevation', 'Weather', 'Temperature', 'Wind', 'Coordinates']] = \\\n",
    "                 name, date, location, beers, moisture, accel, \\\n",
    "                 tech, elevation, weather, temperature, wind, lat_lon\n",
    "\n",
    "    return race_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the race data\n",
    "with open('drive/My Drive/Colab/races.pickle','rb') as read_file:\n",
    "    races = pickle.load(read_file)\n",
    "race_paths = races[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx = len(race_paths)-1\n",
    "\n",
    "for i in range(500, len(race_paths)):\n",
    " \n",
    "    # Initialize a new main df for every 1000 races\n",
    "    if (i%500==0):\n",
    "        all_race_data = pd.DataFrame()\n",
    "    \n",
    "    # Get all the data for an individual race and append to main df\n",
    "    path = race_paths[i]\n",
    "    all_race_data = all_race_data.append(capture_race_data(path))\n",
    "\n",
    "    # Write out the data every 500 races and at the end\n",
    "    if (i in range(499, 8000, 500)) or (i==last_idx):\n",
    "        file_name = 'all_race_data_' + str(i) + '.csv'\n",
    "        all_race_data.to_csv(file_name)\n",
    "        !cp $file_name \"drive/My Drive/Colab\"\n",
    "        \n",
    "    print(i,'\\t',datetime.now().time())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
